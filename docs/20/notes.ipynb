{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Monte Carlo Methods III: Parameter Estimation and Markov Chain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "This hands-on is based on [Gravitational Wave Open Data Workshop 2024](https://github.com/gw-odw/odw-2024/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's make sure we have the necessary packages installed\n",
    "\n",
    "! pip install corner tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# And import all packages at the beginning...\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from corner import corner\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Gravitational waves are ripples in spacetime predicted by Albert Einstein's General Theory of Relativity.\n",
    "These waves propagate outward from sources such as merging black holes, neutron star collisions, or rapidly rotating neutron stars.\n",
    "The first direct detection of gravitational waves, GW150914, was observed by the LIGO detectors in 2015, opening a new window into observing astrophysical phenomena previously inaccessible by electromagnetic observations alone.\n",
    "\n",
    "Detecting and analyzing gravitational waves allows astronomers and physicists to study objects and phenomena that emit very little or no light, providing valuable insights into the behavior of gravity in extreme environments and the properties of dense objects like black holes and neutron stars.\n",
    "\n",
    "In this hands-pon, we introduce Bayesian inference methods for estimating the parameters of gravitational wave signals detected by instruments such as LIGO and Virgo.\n",
    "Specifically, we'll demonstrate parameter estimation using both a rejection method and Markov chain Monte Carlo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "## Bayesian Inference\n",
    "\n",
    "Recalling that Bayesian inference allows us to update our knowledge of model parameters based on observed data.\n",
    "Bayes' theorem mathematically represents this process:\n",
    "\\begin{align}\n",
    "  p(\\theta|d,M) = \\frac{\\mathcal{L}(d|\\theta,M) \\pi(\\theta|M)}{\\mathcal{Z}(d|M)}\n",
    "\\end{align}\n",
    "where:\n",
    "* $p(\\theta|d, M)$ is the posterior distribution.\n",
    "* $\\mathcal{L}(d|\\theta, M)$ is the likelihood function, measuring agreement between model predictions and observed data.\n",
    "* $\\pi(\\theta|M)$ is the prior distribution, representing previous knowledge.\n",
    "* $\\mathcal{Z}(d|M)$ is the evidence, normalizing the posterior.\n",
    "\n",
    "Typically, the posterior cannot be computed analytically, requiring computational approximations.\n",
    "Stochastic (Monte Carlo) sampling is a common method for this purpose."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "## Toy Model\n",
    "\n",
    "We download sample observational data (`toy_model.csv`) containing simulated time-series observations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isfile(\"toy_model.csv\"):\n",
    "    print(\"Downloading toy_model.csv\")\n",
    "    ! wget https://raw.githubusercontent.com/gw-odw/odw-2024/main/Tutorials/Day_3/toy_model.csv\n",
    "else:\n",
    "    print(\"toy_model.csv exists; not downloading\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "Our example data (`toy_model.csv`) contains measurements (`yobs`) recorded at specific times (`time`).\n",
    "Let's visualize these observations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "time, yobs = np.genfromtxt(\"toy_model.csv\", delimiter=\",\").T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(time, yobs)\n",
    "plt.xlabel(\"Time [s]\")\n",
    "plt.ylabel(\"Observed values\")\n",
    "plt.title(\"Simulated Observational Data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "We propose the following sine-Gaussian model to explain the data:\n",
    "\\begin{align}\n",
    "  s(t; f,\\alpha) = e^{-(t/\\alpha)^2} \\sin(2\\pi f t),\n",
    "\\end{align}\n",
    "with frequency parameter $f$ and damping parameter $\\alpha$.\n",
    "To build intuition, we visualize this model for representative parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sinusoid(time, freq):\n",
    "    return np.sin(2 * np.pi * freq * time)\n",
    "\n",
    "def gaussian_exponential(time, alpha):\n",
    "    return np.exp(-(time/alpha)**2)\n",
    "\n",
    "def model_Ms(time, freq, alpha):\n",
    "    return gaussian_exponential(time, alpha) * sinusoid(time, freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "freq, alpha = 2, 0.5\n",
    "\n",
    "plt.plot(time, sinusoid(time, freq), label=\"Sinusoid\")\n",
    "plt.plot(time, gaussian_exponential(time, alpha), label=\"Gaussian envelope\")\n",
    "plt.plot(time, model_Ms(time, freq, alpha), label=\"Sine-Gaussian\")\n",
    "plt.xlabel(\"Time [s]\")\n",
    "plt.ylabel(\"Model components\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "## Parameter Estimation with Rejection Method\n",
    "\n",
    "With the data (`yobs`) and our model $M_s$, we can now estimate the parameters $f$ and $\\alpha$.\n",
    "To do this, similar to previous lectures, we will use Bayes theorem, i.e. we want to approximate the distribution\n",
    "\\begin{align}\n",
    "p(\\theta | d, M_s) = \\frac{\\mathcal{L}(d| \\theta, M_s) \\;\\pi(\\theta | M_s)}{\\mathcal{Z}(d | M_s)}\n",
    "\\end{align}\n",
    "where $\\theta=\\{f, \\alpha\\}$ is the two-dimensional parameter vector and $d$ is `yobs` (measured at times `time`).\n",
    "\n",
    "To this end, we need to define the likelihood and priors.\n",
    "Note that, if we are **only interested in the shape of the distribution**, then we can ignore the evidence, i.e. we can estimate the unnormalized distribution\n",
    "\\begin{align}\n",
    "  p(\\theta | d, M_s) \\propto \\mathcal{L}(d| \\theta, M_s) \\;\\pi(\\theta | M_s)\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "#### Parameter Estimation: Likelihood\n",
    "\n",
    "For this toy example, we will assume that the data consists of the generate model $M_s$ and additive white Gaussian noise, i.e.\n",
    "\\begin{align}\n",
    "y_{\\rm obs}(t) = s(t; f, \\alpha) + \\epsilon\n",
    "\\end{align}\n",
    "where $\\epsilon \\sim N(0, \\sigma)$ by which we mean that $\\epsilon$ is drawn from a Gaussian distribution with zero mean and standard deviation $\\sigma=0.1$ (for now, we will assume this is known a priori, but see challenges below for how it could be estimated). \n",
    "\n",
    "This definition of how the data was created allows us to define our likelihood.\n",
    "Namely, given a value of $\\{f, \\alpha\\}$, the likelihood of a single data point $y_i$ (measured at $t_i$) is:\n",
    "\\begin{align}\n",
    "\\mathcal{L}(y_i| f, \\alpha, M_s) = \\frac{1}{\\sqrt{2\\pi \\sigma^2}} \\exp\\left(-\\frac{(y_i - s(t_i; f, \\alpha))^2}{2\\sigma^2}\\right)\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "To extend this to multiple data points, we assume they are independent then\n",
    "\\begin{align}\n",
    "\\mathcal{L}(y_{obs} | f, \\alpha, M_s) = \\prod_i \\mathcal{L}(y_i| f, \\alpha, M_s)\n",
    "\\end{align}\n",
    "In practice, it is wise to work with the logarithm of the likelihood to avoid numerical overflow.\n",
    "Then, we have that\n",
    "\\begin{align}\n",
    "\\log \\mathcal{L}(y_{obs} | f, \\alpha, M_s) = \\sum_{i} -\\frac{1}{2}\\left(\\frac{\\left(y_i - s(t_i; f, \\alpha)\\right)^2}{\\sigma^2} + \\log\\left(2\\pi \\sigma^2\\right)\\right)\n",
    "\\end{align}\n",
    "We now transcribe this into `python`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_likelihood_Ms(time, yobs, freq, alpha, sigma=0.1):\n",
    "    prediction = model_Ms(time, freq, alpha)\n",
    "    res  = yobs - prediction \n",
    "    logl = -0.5 * (((res/sigma)**2) + np.log(2 * np.pi * sigma**2))\n",
    "    return np.sum(logl, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "#### Parameter Estimation: Priors\n",
    "\n",
    "The second part of Bayes theorem is the *prior*.\n",
    "For our two-component model, we will use a simple disjoint prior (i.e. $\\pi(\\theta | M_s)=\\pi(f| M_s)\\pi(\\alpha | M_s)$) with\n",
    "\\begin{align}\n",
    "  \\pi(f     | M_s) &= \\textrm{Uniform}(0, 5) \\\\\n",
    "  \\pi(\\alpha| M_s) &= \\textrm{Uniform}(0, 1)\n",
    "\\end{align}\n",
    "Let us create a python function to calculate the log of the prior:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_prior_Ms(freq, alpha):\n",
    "    \"\"\" Calculate the log prior under the Ms model\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    freq: array or float\n",
    "        The frequency at which to calculate the prior\n",
    "    alpha: array or float\n",
    "        The alpha at which to calculate the prior\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    log_prior: array\n",
    "        The log_prior calculated for all freq, alpha samples\n",
    "    \"\"\"\n",
    "    # Convert freq, alpha to numpy arrays\n",
    "    freq  = np.atleast_1d(freq)\n",
    "    alpha = np.atleast_1d(alpha)\n",
    "    \n",
    "    # Apply Uniform priors: calculate idxs of array where f, alpha in range\n",
    "    f_min = 0\n",
    "    f_max = 5\n",
    "    f_idxs = (freq > f_min) * (freq < f_max)\n",
    "    \n",
    "    alpha_min = 0\n",
    "    alpha_max = 1\n",
    "    alpha_idxs = (alpha > alpha_min) * (alpha < alpha_max)\n",
    "    \n",
    "    idxs = alpha_idxs * f_idxs\n",
    "    \n",
    "    log_prior_volume = np.log(1/(f_max - f_min) * (1 / (alpha_max - alpha_min)))\n",
    "    \n",
    "    log_prior = np.zeros_like(freq)\n",
    "    log_prior[idxs] = log_prior_volume\n",
    "    log_prior[~idxs] = -np.inf\n",
    "    \n",
    "    return log_prior"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "#### Parameter Estimation: Rejection Sampling\n",
    "\n",
    "Now that we have our likelihood and prior, we will introduce **stochastic (i.e., Monte Carlo) sampling**.\n",
    "We start by using the simplest type of stochastic sampling, rejection sampling.\n",
    "The idea is that to draw samples from a target distribution $p(\\theta | d, M_s)$ which is difficult to sample from, we first generate samples from a generating distribution $g(\\theta$) which is easy to sample from and then weight the samples relative to the target distribution.\n",
    "In practice you can choose any generating distribution you like, but we will use $g(\\theta) = g(f)g(\\alpha)$ where\n",
    "\\begin{align}\n",
    "  g(f)      &= \\textrm{Uniform}(1.8, 2.2) \\\\\n",
    "  g(\\alpha) &= \\textrm{Uniform}(0.2, 0.6)\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "Our rejection sampling algorithm then proceeds as follows:\n",
    "\n",
    "1. Draw $\\theta'=[f, \\alpha]$ from $g(f)$ and $g(\\alpha)$\n",
    "2. Calculate the probability under the target and generating distributions (i.e. $p(\\theta' | d, M_s)$ and $g(\\theta')$)\n",
    "3. Calculate the weight $w=p(\\theta' | d, M_s) / g(\\theta')$\n",
    "4. Draw a random number $u$ uniformly distributed in $[0, 1]$\n",
    "5. If $w > u$, append $\\theta'$ to a set of samples, otherwise reject it and repeat\n",
    "\n",
    "Continue this loop until an acceptable number of samples have been drawn.\n",
    "The resulting set of samples are then an approximation to $p(\\theta | d, M_s)$ and be used to produce summary statistics or create plots. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "We now program the algorithm for our test data.\n",
    "However, there are two important differences between this algorithm and the expression above:\n",
    "\n",
    "1. We will work with the unnormalised distribution $p(\\theta | d, M_s)$ (i.e. we don't calculate the evidence $\\mathcal{Z}$).\n",
    "   As a result, $w$ is also unnormalised and so it needs to be normalised before we apply step 5.\n",
    "   Fortunately, we can normalize $w$ once we have a distribution of values.\n",
    "2. For computational efficiency, rather than using a while loop we will instead draw a set of 100000 samples, calculate the weights for each, and then apply rejection sampling.\n",
    "   This utilises numpy array optimization and also enables us to normalise the weights to a distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rejection_sampler(time, yobs, n_total=100_000):\n",
    "\n",
    "    # Draw n_total samples from g(theta)\n",
    "    freq_gsamples  = np.random.uniform(1.8, 2.2, n_total)\n",
    "    alpha_gsamples = np.random.uniform(0.2, 0.6, n_total)\n",
    "\n",
    "    # Make time a 2D array to enable broadcasting across the samples\n",
    "    time_array = time[:, np.newaxis]\n",
    "    yobs_array = yobs[:, np.newaxis]\n",
    "\n",
    "    # Calculate the log_likelihood and log_prior for all samples\n",
    "    log_post = log_likelihood_Ms(time_array, yobs_array, freq_gsamples, alpha_gsamples) + log_prior_Ms(freq_gsamples, alpha_gsamples)\n",
    "\n",
    "    # Calculate the weights\n",
    "    weights = np.exp(log_post)\n",
    "\n",
    "    # Normalise the weights\n",
    "    weights = weights / max(weights)\n",
    "\n",
    "    # Rejection sample\n",
    "    keep = weights > np.random.uniform(0, 1, weights.shape)\n",
    "    alpha_samples = alpha_gsamples[keep]\n",
    "    freq_samples  = freq_gsamples[keep]\n",
    "\n",
    "    efficiency = len(freq_samples) / len(freq_gsamples)\n",
    "    print(f'Efficiency: {efficiency}')\n",
    "    \n",
    "    return np.array([freq_samples, alpha_samples]).T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "The end result of this is a set of samples `freq_samples` and `alpha_samples` that approximate the posterior distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = rejection_sampler(time, yobs, n_total=100_000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26",
   "metadata": {},
   "source": [
    "We can get a quick visualisation of these by using the `corner` package:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a corner plot\n",
    "\n",
    "fig = corner(samples, bins=20, labels=[\"f\", \"alpha\"], show_titles=True, quantiles=[0.16, 0.5, 0.84])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28",
   "metadata": {},
   "source": [
    "The plot above shows two 1D histograms (one for each parameter) and one 2D histogram (showing any correlations between the samples).\n",
    "Areas where the posterior is large (i.e. the histogram count is high) represent the most probable values of $f$ and $\\alpha$ which explain the data.\n",
    "\n",
    "The samples can also be used to provide a summary statistic.\n",
    "For example, if you wanted to report the mean and standard deviation interval for $f$, you could do something like this: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_samples, alpha_samples = samples.T\n",
    "\n",
    "freq_mean = np.mean(freq_samples)\n",
    "freq_std  = np.std (freq_samples)\n",
    "print(f\"We estimate the the mean and standard deviation of frequency to be {freq_mean:0.2f}+/-{freq_std:0.2f}\")\n",
    "\n",
    "alpha_mean = np.mean(alpha_samples)\n",
    "alpha_std  = np.std (alpha_samples)\n",
    "print(f\"We estimate the the mean and standard deviation of alpha to be {alpha_mean:0.2f}+/-{alpha_std:0.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30",
   "metadata": {},
   "source": [
    "Typically, in GW astronomy, we use the median and a 90\\% credible interval because the posterior is often non-symmetric."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31",
   "metadata": {},
   "source": [
    "In this exerise, we have learned that rejection sampling can be used to approximate the posterior distribution.\n",
    "However, we should note that it is highly inefficienct.\n",
    "It works okay here, because we tightly tuned the edges of $g(\\theta)$, but if you go back and increase these to a wider range, you'll see the efficiency quickly drops off.\n",
    "Moreover, the efficiency of rejection sampling also suffers when we start to look at problems in more than 2 parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32",
   "metadata": {},
   "source": [
    "## Parameter Estimation with Markov Chain Monte Carlo\n",
    "\n",
    "The rejection sampling algorithm is inefficient in problems where the posterior is small compared to the prior volume.\n",
    "To address this, a range of stochastic sampling algorithms are available.\n",
    "In GW astronomy, two are preferred: Markov Chain Monte Carlo (MCMC), and Nested Sampling.\n",
    "\n",
    "We will implement our own simple Metropolis-Hastings MCMC algorithm here:\n",
    "\n",
    "1. Start from an initial guess for the parameters, $\\theta = [f, \\alpha]$.\n",
    "2. Propose new parameters $\\theta'$ by adding a random perturbation drawn from a proposal distribution (often a Gaussian centered around the current parameters).\n",
    "3. Calculate the posterior probabilities $p(\\theta | d)$ and $p(\\theta' | d)$.\n",
    "4. Calculate the acceptance ratio $a = p(\\theta' | d)/p(\\theta | d)$.\n",
    "5. Draw a random number $u$ uniformly distributed between $[0,1]$.\n",
    "6. If $a > u$, accept the proposed parameters $\\theta'$ as the new state; otherwise, retain the current state.\n",
    "7. Repeat steps 2-6 until the desired number of samples is generated.\n",
    "\n",
    "The resulting set of samples approximates the posterior distribution and can be used for parameter estimation and plotting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mcmc_sampler(time, yobs, widths, n_steps=2_500):\n",
    "\n",
    "    freq  = np.random.uniform(1.8, 2.2)\n",
    "    alpha = np.random.uniform(0.2, 0.6)\n",
    "    \n",
    "    log_post = log_likelihood_Ms(time, yobs, freq, alpha) + log_prior_Ms(freq, alpha)\n",
    "\n",
    "    samples = []\n",
    "    progbar = tqdm(total=n_steps)\n",
    "    n_total = 0\n",
    "    while len(samples) < n_steps:\n",
    "        # propose new parameters\n",
    "        freq_proposed  = np.random.normal(freq,  widths[0])\n",
    "        alpha_proposed = np.random.normal(alpha, widths[1])\n",
    "\n",
    "        # calculate new posterior\n",
    "        log_post_proposed = log_likelihood_Ms(time, yobs, freq_proposed, alpha_proposed) + log_prior_Ms(freq_proposed, alpha_proposed)\n",
    "\n",
    "        # Metropolis acceptance criterion\n",
    "        acceptance_prob = np.exp(log_post_proposed - log_post)\n",
    "\n",
    "        if acceptance_prob > np.random.rand():\n",
    "            freq, alpha = freq_proposed, alpha_proposed\n",
    "            log_post    = log_post_proposed\n",
    "            samples.append([freq, alpha])\n",
    "            progbar.update(1)\n",
    "\n",
    "        n_total += 1\n",
    "\n",
    "    print(f'Efficiency: {n_steps/n_total}')\n",
    "    return np.array(samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34",
   "metadata": {},
   "source": [
    "We can now run `mcmc_sampler()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = mcmc_sampler(time, yobs, [0.02, 0.02])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36",
   "metadata": {},
   "source": [
    "The efficiency is much bettter!!!\n",
    "What would happen to the efficiency if we adjust the widths?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37",
   "metadata": {},
   "source": [
    "The `mcmc_sampler` functions returns an numpy array `samples`.\n",
    "This contains all the interesting information that we might want to use.\n",
    "We can create the corner plot as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = corner(samples, bins=20, labels=[\"f\", \"alpha\"], show_titles=True, quantiles=[0.16, 0.5, 0.84])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39",
   "metadata": {},
   "source": [
    "Similar to the rejection method, we can compute some statistics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40",
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_samples, alpha_samples = samples.T\n",
    "\n",
    "freq_mean = np.mean(freq_samples)\n",
    "freq_std  = np.std (freq_samples)\n",
    "print(f\"We estimate the the mean and standard deviation of frequency to be {freq_mean:0.2f}+/-{freq_std:0.2f}\")\n",
    "\n",
    "alpha_mean = np.mean(alpha_samples)\n",
    "alpha_std  = np.std (alpha_samples)\n",
    "print(f\"We estimate the the mean and standard deviation of alpha to be {alpha_mean:0.2f}+/-{alpha_std:0.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41",
   "metadata": {},
   "source": [
    "## Next Step\n",
    "\n",
    "The Gravitational Wave Open Data Workshop has code for processing actual LIGO data.\n",
    "Specifically, [this tutorial](https://github.com/gw-odw/odw-2024/blob/main/Tutorials/Day_3/Tuto_3.2_Parameter_estimation_for_compact_object_mergers.ipynb).\n",
    "However, the code will take at least 30 minutes to run so we will not do it in class.\n",
    "Please feel free to go through the tutorial yourself."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
